---
title: "ds_job"
output: html_document
---

```{r}
###indeed
library(rvest)
library(stringr)

base <- 'https://www.indeed.com'
url <- "https://www.indeed.com/jobs?as_and=Data+Scientist&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&salary=&radius=25&fromage=any&limit=50&sort=&psf=advsrch"
indeed=read_html(url)
page.links <- indeed %>%
  html_nodes(xpath = '//div[contains(@class,"pagination")]//a') %>%
  html_attr('href')
html=paste0("https://www.indeed.com",indeed %>%html_nodes("h2 a") %>%html_attr('href'))

results=html_nodes(indeed,".result")
results_tmp=strsplit(html_text(results),"\n\n")
results=data.frame(
  title=unlist(lapply(results_tmp, function(x) x[2])),
  company=gsub("\n","",unlist(lapply(results_tmp, function(x) ifelse(x[3]=="",x[4],x[3])))),
  experience=unlist(lapply(results_tmp, function(x) ifelse(sum(startsWith(x,"Desired Experience:"))==0,ifelse(sum(startsWith(x,"Experience"))==0,NA,x[startsWith(x,"Experience")]),x[startsWith(x,"Desired Experience:")])))
  ,location_tmp=html_nodes(indeed,".location , #p_68d09e88c4ff1c52 .location span")%>%html_text()
)

for(p in 1:length(page.links)-1){
  
  cat('Moving to Next 50 jobs\n')
  
  # Navigate to next page
  new.page <- read_html(paste0(base, page.links[p]))
  results_tmp=html_nodes(new.page,".result")
  results_tmp=strsplit(html_text(results_tmp),"\n\n")
  html_tmp=paste0("https://www.indeed.com", new.page%>%html_nodes("h2 a") %>%html_attr('href'))
  
  results_tmp_df=data.frame(
    title=unlist(lapply(results_tmp, function(x) x[2])),
    company=gsub("\n","",unlist(lapply(results_tmp, function(x) ifelse(x[3]=="",x[4],x[3])))),
    experience=unlist(lapply(results_tmp, function(x) ifelse(sum(startsWith(x,"Desired Experience:"))==0,ifelse(sum(startsWith(x,"Experience"))==0,NA,x[startsWith(x,"Experience")]),x[startsWith(x,"Desired Experience:")])))
    ,location_tmp=html_nodes(new.page,".location , #p_68d09e88c4ff1c52 .location span")%>%html_text()
  )
results=rbind(results,results_tmp_df)
html=c(html,html_tmp)
Sys.sleep(2)
}


html=unique(html)
results=results[!duplicated(results),]


results$location=gsub("\\(.+|[0-9]","",as.character(results$location_tmp))
results=results[,-4]

results1=list()
for(i in 1:length(html)){
  results1[i]=tryCatch({read_html(html[i])%>%html_text()},error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  Sys.sleep(2)
}

####need to clean results1 
results1=results1[!unlist(lapply(results1,function(x) length(x)==0))]
tmp=sapply(results1,function(x) tryCatch({str_split(x,"\n")[[1]][1]},error=function(e){cat(conditionMessage(e),"\n")}))
results2=sapply(results1, function(x) str_replace_all(x, "[^[:alnum:]]", " "))
results2_title=str_extract(x,str_replace_all(results$title, "[^[:alnum:]]", " "))




###stack_overflow
ds=data.frame(title="",company="",location="",experience="",html="")

for(i in 1:7){
url=paste0("https://stackoverflow.com/jobs?sort=i&q=data+science&pg=",i)
stackoverflow=read_html(url)
results=html_nodes(stackoverflow,".-job-summary")
results_tmp=sapply(strsplit(html_text(results),"\r\n"),str_trim)%>%sapply(function(x) x[x!=""])
results1=data.frame(
  title=unlist(lapply(results_tmp, function(x) x[1])),
  company=unlist(lapply(results_tmp, function(x) x[3])),
  location=unlist(lapply(results_tmp, function(x) x[5])),
  experience=unlist(lapply(results_tmp, function(x) x[length(x)]))
)
html0=unlist(lapply(lapply(stackoverflow%>%html_nodes("h2 a")%>%html_attrs(),function(x) strsplit(as.character(x),'\"')),function(x)x[2]))
results1$html=paste0("https://stackoverflow.com",html0)
ds=rbind(ds,results1)
Sys.sleep(5)}
ds=ds[-1,]

##analyst job
ds1=data.frame(title="",company="",location="",experience="",html="")

for(i in 1:7){
  url=paste0("https://stackoverflow.com/jobs?sort=i&q=analyst&pg=",i)
  stackoverflow=read_html(url)
  results=html_nodes(stackoverflow,".-job-summary")
  results_tmp=sapply(strsplit(html_text(results),"\r\n"),str_trim)%>%sapply(function(x) x[x!=""])
  results1=data.frame(
    title=unlist(lapply(results_tmp, function(x) x[1])),
    company=unlist(lapply(results_tmp, function(x) x[3])),
    location=unlist(lapply(results_tmp, function(x) x[5])),
    experience=unlist(lapply(results_tmp, function(x) ifelse(startsWith(x[length(x)],"Be one"),x[length(x)-1],x[length(x)])
  )))
  html0=unlist(lapply(lapply(stackoverflow%>%html_nodes("h2 a")%>%html_attrs(),function(x) strsplit(as.character(x),'\"')),function(x)x[2]))
  results1$html=paste0("https://stackoverflow.com",html0)
  ds1=rbind(ds1,results1)
  Sys.sleep(5)}

ds1=ds1[-1,]
ds_full=rbind(ds,ds1)
ds_full$html=as.character(ds_full$html)
industry=list()
qualification=list()
for (i in 1:length(ds_full$html)){
  jd=read_html(ds_full$html[i])
  tmp=as.vector((unlist(sapply((strsplit(html_text(html_nodes(jd,".-about-job")),"\r\n")),str_trim))))
  tmp1=tmp[tmp!=""]
  industry[i]=ifelse(length(which(tmp1=="Industry:"))==0,"No industry listed",tmp1[which(tmp1=="Industry:")+1])
  tmp2=html_text(html_nodes(jd,".-skills-requirements , .-job-description"))
  qualification[i]=ifelse(length(tmp2)>1,paste(tmp2,collapse = '\"'),tmp2)
  Sys.sleep(2)
}

ds_full$industry=unlist(industry)
ds_full$qualification=unlist(qualification)
ds_full$company
write.csv(ds_full,file="data_science.csv")


##merck
url="https://jobs.msd.com/search/?q=data+science&q2=&title=&location=US&date="
base="https://jobs.msd.com"
merck=read_html(url)
page.links <- paste0(base,"/search/",merck %>%html_nodes(xpath = '//div[contains(@class,"pagination")]//a') %>%
                       html_attr('href'))
merck_results=data.frame(title="",location="",html="")
for(i in 1:length(page.links)){
  cat('Moving to Next Page\n')
  
  # Navigate to next page
  new.page <- read_html(page.links[i])
  tmp=data.frame(
    title=html_nodes(new.page,".jobTitle-link")%>%html_text(),
    location=trimws(html_nodes(new.page,".colLocation")%>%html_text()),
    html=paste0(base,html_nodes(new.page,".jobTitle-link")%>%html_attr("href"))
  )
  merck_results=rbind(merck_results,tmp)
  Sys.sleep(2)
}


merck_results=merck_results[-1,]
merck_results=merck_results[!duplicated(merck_results),]
desc=list()
for(i in 1:length(merck_results$html)){
  cat("Move to next joblink\n")
  desc[i]=tryCatch({read_html(as.character(merck_results$html[i]))%>%html_nodes(".job")%>%html_text()},error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  Sys.sleep(2)
}
ind=1:dim(merck)[1]
set.seed(123)
merck_results=merck_results[sample(ind,15),]

###sanofi
url="https://jobs.sanofi.us/search-jobs/data%20science/507-18104/1"
sanofi=read_html(url)
title=html_nodes(sanofi,"a h2")%>%html_text()
html=html_nodes(sanofi,"a")%>%html_attr("href")
html=paste0("https://jobs.sanofi.us",html[startsWith(html,"/job/")])[1:length(title)]
location=html_nodes(sanofi,"span")%>%html_text()
location=location[1:length(title)]
sanofi_results=data.frame(title=title,location=location,html=html)


desc=list()
for(i in 1:length(sanofi_results$html)){
  cat("Move to next joblink\n")
  desc[i]=tryCatch({read_html(as.character(sanofi_results$html[i]))%>%html_nodes(".ats-description")%>%html_text()},error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  Sys.sleep(2)
}
###roche
url="https://www.roche.com/careers/jobs/jobsearch.htm?limit=50&keywords=data%20science&countryCodes=US&jobTypeCodes=STANDARD&offset=3"
base="https://www.roche.com"
roche=read_html(url)
title=html_nodes(roche,"#results-table a")%>%html_text()
html=paste0(base,html_nodes(roche,"#results-table a")%>%html_attr("href"))


roche_results=data.frame(title=title,html=html)
desc=list()
for(i in 1:length(roche_results$html)){
  cat("Move to next joblink\n")
  desc[i]=tryCatch({paste(read_html(as.character(roche_results$html[i]))%>%html_nodes("li")%>%html_text(),collapse="\n\t")},error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  Sys.sleep(2)
}

roche_results$desc=unlist(desc)
write.csv(roche_results,"roche_results.csv")
```